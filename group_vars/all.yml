---
ceph_origin: repository
ceph_repository: rhcs
ceph_rhcs_version: 4
ceph_iscsi_config_dev: false
rbd_cache: "true"
rbd_cache_writethrough_until_flush: "false"
rbd_client_directories: false # this will create rbd_client_log_path and rbd_client_admin_socket_path directories with proper permissions
monitor_interface: ens192
journal_size: 1024 # OSD journal size in MB
public_network: 143.160.92.0/22
cluster_network: 143.160.92.0/22
ceph_conf_overrides:
   global:
      mon_clock_drift_allowed: 0.5
      mon_pg_warn_min_per_osd: 0
      mon osd down out interval: 28800 # 8 hours
      mon max pg per osd: 400
      osd pool default size: 3
      osd pool default min size: 2
      filestore rocksdb options: "max_background_compactions=8;compaction_readahead_size=2097152;compression=kNoCompression;compact_on_mount=false"
      filestore omap backend: "rocksdb"
      mon warn pg max object skew: "-1"
   osd:
     osd_memory_target: 12884901888
     osd_client_watch_timeout: 15
     osd_heartbeat_grace: 20
     osd_heartbeat_interval: 5
     osd scrub begin hour: 22
     osd scrub end hour: 5
     ms async op threads: 6 # default=3
     ms async max op threads: 10 # default=5
     osd op num threads per shard hdd: 8 # default=1
     osd op num threads per shard ssd: 8 # default=2
     osd scrub min interval:   172800 # 3 days instead of 1 day by default
     osd scrub max interval:   640800 # 2 weeks instead of 1 week by default

upgrade_ceph_packages: True
ceph_docker_image: "rhceph/rhceph-5-rhel8"
ceph_docker_image_tag: "latest"
ceph_docker_registry: "redhatregistry.nwu.ac.za"
#ceph_docker_registry: "registry.redhat.io"
ceph_docker_registry_auth: true

#ceph_docker_registry_username: '12513764|senestian-rhcs'
#ceph_docker_registry_password: eyJhbGciOiJSUzUxMiJ9.eyJzdWIiOiIwYTU3ZjA3YmVjYzM0MDliOTk3MTI5ZmJkYjRkYjY5NyJ9.lm3tLWKv7iu14glXumTLU-jTmUb51OyTasocEhaWQZ9V-5R0SIHJkI-bAQkmmRw5SrzY71Ko2xfNMJeNiqdbAj4_C_s_UjJ8sykMS2EvCHCQz0OjfZ7fWQb6r7_keJZrwm3_Z4Y3BKzpOGOiYGe-HFfgXy66zkQDNUgo3mr7glID4HzCy78PvV2uMj-qRTDIO5XuYZPWJrubaE9jU099lB2uOv2ePbzQ6mWl0j-LVUmS2rH-k_RukQ-aZd2eM_12beZk9VV7byvlGGQS3K6UC7YTGHp_80A6VYnXy24QtNDdko-nClIykt8w3KTFiFZEPbVLlaAeDYosqKe86Xe0Yss9c4WNyT0V-Zc69g7YmL4vXn4FKn_sJHa8zUcwe5vRbbw9UBynrNfrP_AvV6IfVCkJkj1fVMToaGA2heFd-_dKQqJumcBsKSDMKCTWq0Tjw9lTnaLrdYGooyGSky2W-OblznRpexQ4VZ593P4NDbJ4k67vkKxoivDOux9sdLykLus3V6s4p-GCWvo-2moql1peDnYK1uwvdDReO_UrwYbysq8CNHWSipcLN37Bc6bPTcSkX5GhkWNfwaGiS54Y4CP37Q7zG2yRKApvZpyZ3qDW7bNRJIk-ntlnt3yVFv3xnoTSiOM8oqxqxC4Guvi23RL3Q7uh0eKaRUy99pkumrg

ceph_docker_http_proxy: "http://143.160.36.154:80"
ceph_docker_https_proxy: "http://143.160.36.154:80"
ceph_docker_no_proxy: "localhost,127.0.0.1,143.160.0.0/16,redhatregistry.nwu.ac.za"
containerized_deployment: true
ceph_docker_registry_username: 'jimmy'
ceph_docker_registry_password: e9AJ64QF6mPxMW62JD5JzY9

#grafana_container_image: "redhatregistry.nwu.ac.za/rhceph/rhceph-4-dashboard-rhel8:latest"
#prometheus_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus:latest"
#alertmanager_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus-alertmanager:latest"
#node_exporter_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus-node-exporter:latest"

grafana_container_image: "redhatregistry.nwu.ac.za/rhceph/rhceph-5-dashboard-rhel8:5"
prometheus_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus:v4.6"
alertmanager_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus-alertmanager:v4.6"
node_exporter_container_image: "redhatregistry.nwu.ac.za/openshift4/ose-prometheus-node-exporter:v4.6"


#############
# DASHBOARD #
#############
#dashboard_enabled: True
# Choose http or https
# For https, you should set dashboard.crt/key and grafana.crt/key
# If you define the dashboard_crt and dashboard_key variables, but leave them as '',
# then we will autogenerate a cert and keyfile
#dashboard_protocol: https
#dashboard_port: 8443
dashboard_admin_user: admin
#dashboard_admin_user_ro: false
# This variable must be set with a strong custom password when dashboard_enabled is True
dashboard_admin_password: redhat
# We only need this for SSL (https) connections
#dashboard_crt: ''
#dashboard_key: ''
#dashboard_certificate_cn: ceph-dashboard
#dashboard_tls_external: false
#dashboard_grafana_api_no_ssl_verify: "{{ true if dashboard_protocol == 'https' and not grafana_crt and not grafana_key else false }}"
#dashboard_rgw_api_user_id: ceph-dashboard
#dashboard_rgw_api_admin_resource: ''
#dashboard_rgw_api_no_ssl_verify: False
#dashboard_frontend_vip: ''
#node_exporter_container_image: registry.redhat.io/openshift4/ose-prometheus-node-exporter:latest
#prometheus_frontend_vip: ''
#alertmanager_frontend_vip: ''
#node_exporter_port: 9100
grafana_admin_user: admin
# This variable must be set with a strong custom password when dashboard_enabled is True
grafana_admin_password: redhat
# We only need this for SSL (https) connections
#grafana_crt: ''
#grafana_key: ''
#grafana_container_image: registry.redhat.io/rhceph/rhceph-4-dashboard-rhel8:latest
# When using https, please fill with a hostname for which grafana_crt is valid.
#grafana_server_fqdn: ''
#grafana_container_cpu_period: 100000
#grafana_container_cpu_cores: 2
# container_memory is in GB
#grafana_container_memory: 4
#grafana_uid: 472
#grafana_datasource: Dashboard
#grafana_dashboards_path: "/etc/grafana/dashboards/ceph-dashboard"
#grafana_dashboard_version: nautilus
#grafana_dashboard_files:
#  - ceph-cluster.json
#  - cephfs-overview.json
#  - host-details.json
#  - hosts-overview.json
#  - osd-device-details.json
#  - osds-overview.json
#  - pool-detail.json
#  - pool-overview.json
#  - radosgw-detail.json
#  - radosgw-overview.json
#  - rbd-overview.json
#grafana_plugins:
#  - vonage-status-panel
#  - grafana-piechart-panel
#grafana_allow_embedding: True
#grafana_port: 3000
#prometheus_container_image: registry.redhat.io/openshift4/ose-prometheus:latest
#prometheus_container_cpu_period: 100000
#prometheus_container_cpu_cores: 2
# container_memory is in GB
#prometheus_container_memory: 4
#prometheus_data_dir: /var/lib/prometheus
#prometheus_conf_dir: /etc/prometheus
#prometheus_user_id: '65534'  # This is the UID used by the prom/prometheus container image
#prometheus_port: 9092
#prometheus_conf_overrides: {}
# Uncomment out this variable if you need to customize the retention period for prometheus storage.
# set it to '30d' if you want to retain 30 days of data.
#prometheus_storage_tsdb_retention_time: 15d
#alertmanager_container_image: registry.redhat.io/openshift4/ose-prometheus-alertmanager:latest
#alertmanager_container_cpu_period: 100000
#alertmanager_container_cpu_cores: 2
# container_memory is in GB
#alertmanager_container_memory: 4
#alertmanager_data_dir: /var/lib/alertmanager
#alertmanager_conf_dir: /etc/alertmanager
#alertmanager_port: 9093
#alertmanager_cluster_port: 9094
# igw
#
# `igw_network` variable is intended for allowing dashboard deployment with iSCSI node not residing in the same subnet than what is defined in `public_network`.
# For example:
# If the ceph public network is 2a00:8a60:1:c301::/64 and the iSCSI Gateway resides
# at a dedicated gateway network (2a00:8a60:1:c300::/64) (With routing between those networks).
# It means "{{ hostvars[item]['ansible_facts']['all_ipv4_addresses'] | ips_in_ranges(public_network.split(',')) | last | ipwrap }}" will be empty.
# As a consequence, this prevent from deploying dashboard with iSCSI node when it reside in a subnet different than `public_network`.
# Using `igw_network` make it possible, set it with the subnet used by your iSCSI node.
#igw_network: "{{ public_network }}"

radosgw_interface: ens192


os_tuning_params:
  - { name: kernel.pid_max, value: 4194303 }
  - { name: fs.file-max, value: 26234859 }
  - { name: vm.zone_reclaim_mode, value: 0 }
  - { name: vm.swappiness, value: 10 }
  - { name: vm.min_free_kbytes, value: "{{ vm_min_free_kbytes }}" }
  - { name: net.core.rmem_max, value: 56623104 }
  - { name: net.core.wmem_max, value: 56623104 }
  - { name: net.core.rmem_default, value: 56623104 }
  - { name: net.core.wmem_default, value: 56623104 }
  - { name: net.core.optmem_max, value: 40960 }
  - { name: net.core.somaxconn, value: 1024 }
  - { name: net.core.backlog, value: 50000 }
  - { name: net.ipv4.tcp_mem, value: 4096 87380 56623104 }
  - { name: net.ipv4.tcp_rmem, value: 4096 87380 56623104 }
  - { name: net.ipv4.tcp_wmem, value: 4096 87380 56623104 }
  - { name: net.ipv4.tcp_fin_timeout, value: 20 }

